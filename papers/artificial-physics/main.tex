\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{finding}{Finding}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]

\title{Artificial Physics: \\
Evolutionary Emergence of Quantum Structures \\
in Resource-Constrained DSL Competition}

\author{
  Hiroshi Kohashiguchi\\
  Independent Researcher\\
  Tokyo, Japan
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
We present a computational framework for understanding the emergence of 
quantum-like structures through evolutionary competition of Domain-Specific 
Languages (DSLs) under resource constraints. Using \textbf{dynamic task 
evaluation}---where graph size $N \sim U(3,10)$ and steps $k \sim U(2,6)$ 
vary randomly---we prevent scalar DSLs from ``memorizing'' fixed solutions. 
Our experiments with $N=100$ population and 10 runs show that Matrix DSLs 
achieve 100\% dominance within $3.8 \pm 1.7$ generations (95\% CI: [2.6, 5.0]), 
providing evidence for the \emph{Substrate Hypothesis}. We further investigate 
spontaneous emergence of matrix operations from scalar-only DSLs across 
500 generations (Experiment 2A), finding that matrix operations \textbf{never 
emerged} (0/5 runs) despite scalar DSLs achieving only 60--80\% success on 
dynamic tasks. When matrix operations are injected (Experiment 2B), success 
rates jump from 70--80\% to 100\% ($+24.0\% \pm 5.5\%$), demonstrating the 
decisive advantage of matrix structure for generalization.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

The question ``Why is the universe quantum mechanical?'' has puzzled physicists 
and philosophers since the inception of quantum theory. Previous work has 
established that quantum structure cannot be derived from classical computational 
models such as SK combinatory logic \cite{kohashiguchi2024independence} or 
reversible computation \cite{kohashiguchi2024limits}. Furthermore, 
\cite{kohashiguchi2024axioms} identified axiom A1 (state space extension / 
superposition) as the unique primitive axiom required for quantum mechanics, 
while \cite{kohashiguchi2024naturalness} demonstrated that quantum mechanics 
has minimal description length on a quantum substrate.

This work builds upon a rich tradition of \emph{digital physics}---the idea 
that the universe is fundamentally computational. Wolfram's ``A New Kind of 
Science'' \cite{wolfram2002} and subsequent physics project \cite{wolfram2020} 
explored cellular automata and hypergraphs as computational substrates. 
Lloyd's ``Programming the Universe'' \cite{lloyd2006} framed the universe as 
a quantum computer. In the artificial life community, systems like Avida 
\cite{lenski2003} and genetic programming \cite{koza1992} have demonstrated 
how complex structures can emerge through evolutionary pressure.

In this paper, we approach the foundational question from a novel perspective: 
\emph{evolutionary DSL competition}. We model physical laws as Domain-Specific 
Languages (DSLs) competing for computational resources in a ``parent universe'' 
(Localhost). The selection pressure is \emph{algorithmic naturalness}---laws that 
provide shorter descriptions (lower Kolmogorov complexity $K$) are favored.

This work provides an \emph{evolutionary complement} to our previous results: 
while \cite{kohashiguchi2024axioms, kohashiguchi2024naturalness} established 
the \emph{static} properties of quantum structure (minimality, uniqueness), 
the present paper demonstrates its \emph{dynamic} emergence through selection.

Our key contributions are:

\begin{enumerate}
    \item \textbf{Genesis-Matrix Framework}: A simulation environment for 
    DSL evolution under resource constraints.
    
    \item \textbf{Quantum Dawn Experiment}: Demonstration that Matrix DSLs 
    dominate Scalar DSLs through pure selection pressure.
    
    \item \textbf{Evolution of Operators}: Investigation of spontaneous 
    emergence of matrix operations from scalar primitives.
    
    \item \textbf{Artificial Physics}: A theoretical framework identifying 
    physical laws as optimally compressed DSLs.
\end{enumerate}

%==============================================================================
\section{Theoretical Framework}
%==============================================================================

\subsection{The Substrate Hypothesis}

\begin{definition}[Substrate Hypothesis]
The universe's computational substrate is \emph{quantum-native} ($U_Q$), 
not classical ($U_C$). Formally:
\begin{equation}
    K_{U_Q}(\text{QM}) \ll K_{U_C}(\text{QM})
\end{equation}
where $K_U(L)$ denotes the Kolmogorov complexity of physical law $L$ 
on substrate $U$.
\end{definition}

\subsection{Localhost-Container Model}

We model the ``parent universe'' as a \emph{Localhost} that allocates 
computational resources to \emph{Containers} (child universes).

\begin{definition}[Localhost]
A Localhost $\mathcal{L}$ is a tuple $(M, \{C_i\}, \mathcal{A})$ where:
\begin{itemize}
    \item $M$ is the total available memory. This constraint corresponds 
    to the Bekenstein bound~\cite{bekenstein1981}, which posits a 
    fundamental upper limit on the entropy (information content) that 
    can be contained within a finite region of space.
    \item $\{C_i\}$ is the set of Containers
    \item $\mathcal{A}$ is the resource allocation function
\end{itemize}
\end{definition}

\begin{definition}[Container]
A Container $C$ is a tuple $(P, D, f)$ where:
\begin{itemize}
    \item $P$ is a program (encoding of physical law)
    \item $D$ is the DSL (vocabulary of operations)
    \item $f$ is the fitness score
\end{itemize}
\end{definition}

\begin{remark}[Physical Correspondence]
The Localhost-Container model admits natural physical interpretations, 
as summarized in Table~\ref{tab:correspondence}. Specifically, we map 
the Localhost to a Meta-Universe or Quantum Vacuum~\cite{linde1986}, 
and Containers to Pocket Universes~\cite{guth1981,guth1997} that run 
specific physical laws. The memory constraint $M$ corresponds to the 
Bekenstein Bound~\cite{bekenstein1981}, providing a physical limit on 
information content. These correspondences are suggestive rather than 
rigorous, but serve as the conceptual motivation for using evolutionary 
dynamics---akin to Smolin's Cosmological Natural Selection~\cite{smolin1992,smolin1997}---to 
explain the origin of physical laws.
\end{remark}

\begin{table}[ht]
\centering
\caption{Correspondence between the Localhost-Container Model and Theoretical Physics.}
\label{tab:correspondence}
\small
\begin{tabularx}{\textwidth}{@{}lllX@{}}
\toprule
\textbf{Component} & \textbf{Computational} & \textbf{Physical} & \textbf{Role / Function} \\
\midrule
System & Localhost & Meta-Universe~\cite{linde1986} & Resource provider \\
Unit & Container & Pocket Universe~\cite{guth1981,guth1997} & Isolated spacetime with specific laws \\
Constraint & Memory ($M$) & Bekenstein Bound~\cite{bekenstein1981} & Max.\ information content \\
Laws & DSL & Physical Laws & Vocabulary of interactions \\
State & Program ($P$) & Quantum State & Configuration of reality \\
Selection & $V_{\text{inf}}$ & Cosmological Selection & Expansion mechanism \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Complexity Measures}

We define operational measures for description length and execution time:

\begin{definition}[Description Length $K$]
The description length $K(P)$ of a program $P$ is the number of nodes 
in its Abstract Syntax Tree (AST). This serves as a computable proxy 
for Kolmogorov complexity.
\end{definition}

\begin{definition}[Execution Time $T$]
The execution time $T(P)$ is the wall-clock time (in milliseconds) 
required to evaluate program $P$ on the target task.
\end{definition}

\subsection{Inflation Rate and Selection}

The \emph{inflation rate} determines resource acquisition:

\begin{equation}
    V_{\text{inf}}(C) = \frac{1}{K(P) \cdot T(P)}
\end{equation}

where $K(P)$ is the description length and $T(P)$ is execution time.

\begin{remark}[Choice of Inflation Rate]
We adopt the multiplicative form $V_{\text{inf}} \propto 1/(K \cdot T)$ as the 
simplest separable model that penalizes both complexity and slowness equally. 
Alternative forms such as $1/(aK + bT)$ were considered; preliminary experiments 
suggest that the qualitative conclusions (Matrix DSL dominance) are robust to 
this choice, though a systematic sensitivity analysis is left for future work.
\end{remark}

\begin{proposition}[Selection Pressure]
Containers with lower $K$ have higher fitness and acquire more resources, 
leading to evolutionary dominance.
\end{proposition}

%==============================================================================
\section{Experimental Setup}
%==============================================================================

\subsection{Task: Graph Walk with Dynamic Evaluation}

The experimental task is to count paths from node $0$ to node $N-1$ in 
exactly $k$ steps on a graph with adjacency matrix $A$.

The ground truth is:
\begin{equation}
    \text{answer} = (A^k)_{0, N-1}
\end{equation}

This task naturally exhibits a complexity asymmetry: scalar DSLs must 
enumerate paths individually ($K = O(N)$ or $O(k)$), while matrix DSLs 
can compute $A^k$ directly ($K = O(1)$).

\textbf{Dynamic Task Evaluation.} To prevent scalar DSLs from ``memorizing'' 
solutions for a fixed $(N, k)$, we introduced \emph{dynamic task evaluation}:
\begin{itemize}
    \item Graph size: $N \sim \text{Uniform}(3, 10)$
    \item Steps: $k \sim \text{Uniform}(2, 6)$
    \item Adjacency matrix $A$ regenerated randomly per evaluation
    \item Fitness = average success rate over 10 random instances
\end{itemize}

This ensures that only programs with \emph{structural generalization} 
(i.e., matrix operations) can achieve 100\% success.

\subsection{DSL Species}

\begin{table}[ht]
\centering
\begin{tabular}{lll}
\toprule
Species & Operations & Description Length $K$ \\
\midrule
Scalar & ADD, SUB, MUL, IF & $O(N)$ or $O(k)$ \\
Matrix & MATMUL, MATPOW, GET & $O(1)$ \\
\bottomrule
\end{tabular}
\caption{DSL species and their description lengths for the graph walk task. 
Here $O(\cdot)$ denotes scaling with respect to AST node count.}
\label{tab:dsl}
\end{table}

\subsection{Evolution Parameters}

All experiments use \textbf{dynamic task evaluation} (Section 3.1).

\textbf{Experiment 1 (Matrix vs Scalar Competition):}
\begin{itemize}
    \item Population size: $N = 100$
    \item Generations: 50
    \item Mutation rate: 15\%
    \item Elite preservation: Top 5 individuals
    \item Initial Matrix ratio: 20\%
    \item Fitness tests per individual: 10 random $(N, k, A)$ instances
    \item Random seeds: 10 independent runs (seeds 0--9)
\end{itemize}

\textbf{Experiment 2 (Evolution of Operators):}
\begin{itemize}
    \item \textbf{Experiment 2A (Spontaneous Emergence)}: 500 generations with 
    NO injection, to test whether matrix operations can emerge de novo 
    (5 runs, seeds 0--4).
    \item \textbf{Experiment 2B (Simulated Discovery)}: 500 generations with 
    matrix operations injected at generation 200, to observe post-discovery 
    dynamics (5 runs, seeds 0--4).
    \item Population size: $N = 100$ for both conditions.
    \item Fitness tests per individual: 10 random instances.
\end{itemize}

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Experiment 1: The Quantum Dawn}

\begin{finding}[Matrix Dominance with Dynamic Tasks]
In the graph walk task with \textbf{dynamic evaluation} ($N \sim U(3,10)$, 
$k \sim U(2,6)$), Matrix DSL achieves 100\% dominance within $3.8 \pm 1.7$ 
generations (95\% CI: $[2.6, 5.0]$) across 10 independent runs.
\end{finding}

\textbf{Experimental Evidence:}

\begin{table}[ht]
\centering
\begin{tabular}{ccc}
\toprule
Run & Convergence (Gen) & Final Matrix (\%) \\
\midrule
1 & 8 & 100 \\
2 & 2 & 100 \\
3 & 3 & 100 \\
4 & 4 & 100 \\
5 & 3 & 100 \\
6 & 3 & 100 \\
7 & 4 & 100 \\
8 & 3 & 100 \\
9 & 5 & 100 \\
10 & 3 & 100 \\
\midrule
Mean & 3.8 & 100 \\
SD & 1.7 & -- \\
95\% CI & {[}2.6, 5.0{]} & -- \\
\bottomrule
\end{tabular}
\caption{Experiment 1 (Dynamic): Matrix DSL dominance with dynamic task 
evaluation across 10 runs.}
\label{tab:exp1}
\end{table}

The convergence remains fast and highly reproducible even with dynamic 
tasks---all 10 runs achieved 100\% Matrix dominance within 2--8 generations. 
This demonstrates that Matrix DSL's advantage is \emph{structural}, not 
merely due to memorization of a fixed task.

\subsection{Experiment 2: Evolution of Operators}

\subsubsection{Experiment 2A: Spontaneous Emergence Test}

\begin{finding}[No Spontaneous Emergence]
In Experiment 2A with $N=100$ population, 500 generations, and 
\textbf{dynamic task evaluation} (5 runs, NO injection), matrix operations 
did \textbf{not emerge spontaneously in any run} (0/5). Scalar DSLs 
achieved only 60--80\% success rate on dynamic tasks, yet the ``conceptual 
leap'' to matrix operations never occurred.
\end{finding}

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule
Run & Best Success (\%) & Operators & Matrix Emerged \\
\midrule
1 & 100 & 15 & No \\
2 & 100 & 15 & No \\
3 & 90 & 15 & No \\
4 & 100 & 15 & No \\
5 & 100 & 15 & No \\
\midrule
Mean & $98.0 \pm 4.5$ & -- & -- \\
\bottomrule
\end{tabular}
\caption{Experiment 2A (Dynamic): Spontaneous emergence test (NO injection, 
500 generations). Best success rates are momentary peaks; typical generation 
success was 60--80\%. Matrix operations did not emerge.}
\label{tab:exp2a}
\end{table}

This is central evidence for our claim: despite dynamic task pressure 
(which penalizes memorization), scalar DSLs could not discover matrix 
operations spontaneously. The ``conceptual leap'' from scalar to matrix 
requires the substrate to provide matrix primitives.

\subsubsection{Experiment 2B: Simulated Discovery Test}

\begin{finding}[Success Jump After Injection]
In Experiment 2B with \textbf{dynamic task evaluation}, matrix operations 
were injected at generation 200. Success rate jumped from 70--80\% to 
100\% ($+24.0\% \pm 5.5\%$), demonstrating the decisive advantage of 
matrix structure for generalization.
\end{finding}

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule
Run & Pre-Injection (\%) & Post-Injection (\%) & Jump (\%) \\
\midrule
1 & 70 & 100 & +30 \\
2 & 80 & 100 & +20 \\
3 & 80 & 100 & +20 \\
4 & 80 & 100 & +20 \\
5 & 70 & 100 & +30 \\
\midrule
Mean & -- & -- & $+24.0 \pm 5.5$ \\
\bottomrule
\end{tabular}
\caption{Experiment 2B (Dynamic): Simulated discovery test (injection at 
Gen 200). Clear success jump demonstrates matrix operations' advantage.}
\label{tab:exp2b}
\end{table}

\textbf{Interpretation:} Unlike the fixed-task setting (where scalar DSLs 
could achieve 100\% by ``memorization''), dynamic task evaluation reveals 
the true gap:

\begin{itemize}
    \item \textbf{Scalar solutions}: 60--80\% success, cannot generalize 
    across varying $(N, k)$.
    \item \textbf{Matrix solutions}: 100\% success, structurally general. 
    The program $\texttt{MATPOW(A, k)}$ works for \emph{any} $(N, k)$.
\end{itemize}

The $+24\%$ success jump after injection demonstrates that matrix operations 
provide a \emph{qualitative} advantage in generalization, not merely a 
quantitative improvement.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Implications for the Substrate Hypothesis}

Our results provide evidence consistent with the Substrate Hypothesis:

\begin{enumerate}
    \item \textbf{Matrix DSL dominance}: When both scalar and matrix 
    operations are available, matrix operations dominate due to their 
    superior compression. This is consistent with quantum mechanics being 
    ``natural'' on a quantum substrate.
    
    \item \textbf{Emergence difficulty}: In Experiment 2A, matrix 
    operations did not emerge spontaneously in any of 5 runs over 
    500 generations (0/5), despite 15 compound operators being invented. 
    This suggests matrix structure may need to be ``built into'' the substrate.
    
    \item \textbf{Rapid takeover}: Once matrix operations exist, they 
    achieve 100\% dominance within 2--8 generations across all 10 runs 
    (95\% CI: $[2.6, 5.0]$) even with dynamic tasks---consistent with 
    quantum mechanics being the minimal description on a quantum-native substrate.
    
    \item \textbf{Generalization advantage}: With dynamic task evaluation, 
    matrix operations show a clear $+24\%$ success jump over scalar solutions, 
    demonstrating their structural advantage for generalization.
\end{enumerate}

\subsection{Why Is the Universe Quantum?}

Our framework suggests a conditional answer:

\begin{quote}
\emph{If} the universe's computational substrate is quantum-native, 
\emph{then} quantum mechanics provides the shortest description 
(minimal $K$), and evolutionary selection for minimal $K$ would 
naturally lead to quantum-like physical laws.
\end{quote}

This reframes the question from ``Why quantum?'' to ``What properties 
must the substrate have for quantum mechanics to be minimal?''

\subsection{Artificial Physics}

We define \emph{Artificial Physics} as the study of physical laws as 
evolutionarily selected DSLs:

\begin{definition}[Artificial Physics]
Physical laws are DSLs that have been selected by evolutionary pressure 
to minimize description length $K$ on the underlying computational substrate.
\end{definition}

This perspective reframes fundamental physics questions:
\begin{itemize}
    \item ``Why quantum mechanics?'' $\to$ ``What substrate makes QM minimal?''
    \item ``What is a physical law?'' $\to$ ``An optimally compressed DSL''
    \item ``Why these constants?'' $\to$ ``They minimize $K$ on $U_Q$''
\end{itemize}

\subsection{Limitations and Future Work}

Several limitations of this study should be noted:

\begin{enumerate}
    \item \textbf{Single task}: Our experiments focus on the graph walk 
    task. While we expect similar results for other linear-algebraic tasks, 
    this remains to be verified.
    
    \item \textbf{Idealized model}: The Localhost-Container model abstracts 
    away many physical complexities (noise, decoherence, imperfect observation).
    
    \item \textbf{Limited DSL space}: The DSL search space is constrained 
    by our chosen primitives; richer primitive sets might yield different 
    emergence dynamics.
\end{enumerate}

Future work will address these limitations by: (1) testing additional 
tasks (Markov chains, quantum circuits), (2) exploring richer mutation 
operators that might enable genuine emergence, (3) formalizing the 
connection between the Localhost-Container model and established physics 
(holographic principle, cosmological natural selection~\cite{smolin1992}), 
and (4) exploring the connection to Categorical Quantum Mechanics, as 
our functional perspective on physical laws as DSLs aligns well with 
the diagrammatic reasoning of process theories.

%==============================================================================
\section{Conclusion}
%==============================================================================

We have demonstrated through simulation that:

\begin{enumerate}
    \item Matrix DSLs (quantum-like structures) dominate Scalar DSLs 
    (classical structures) under selection pressure for minimal 
    description length.
    
    \item The dominance is rapid ($3.8 \pm 1.7$ generations; 95\% CI: 
    $[2.6, 5.0]$) even with dynamic task evaluation, and robust across 
    10 independent runs.
    
    \item Spontaneous emergence of matrix operations from scalar 
    primitives did not occur in any of 5 runs over 500 generations 
    with dynamic tasks (Experiment 2A: 0/5), even though scalar DSLs 
    could only achieve 60--80\% success. This suggests that quantum 
    structure may be fundamental rather than emergent.
    
    \item When matrix operations are injected, success rates jump 
    from 70--80\% to 100\% ($+24\% \pm 5.5\%$), demonstrating their 
    decisive advantage for generalization.
\end{enumerate}

These results are consistent with the \textbf{Substrate Hypothesis}: 
if the universe's computational substrate is quantum-native, then quantum 
mechanics would be the ``natural'' physical law on this substrate.

The \textbf{Artificial Physics} framework provides a new lens for 
understanding fundamental physics: physical laws as evolutionarily 
selected, algorithmically optimal descriptions on the computational 
substrate of reality.

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

The author thanks Gregory Chaitin for foundational work on algorithmic 
information theory, and Stephen Wolfram for pioneering the computational 
universe paradigm. All implementations are released as open-source software 
under the MIT License at \url{https://github.com/future-apps-jp/omega/}.

%==============================================================================
\begin{thebibliography}{99}

\bibitem{kohashiguchi2024independence}
H. Kohashiguchi,
``On the Independence of Quantum Structure from SK Combinatory Logic,''
PhilArchive, 2025.

\bibitem{kohashiguchi2024limits}
H. Kohashiguchi,
``Computational Limits of Deriving Quantum Structure from Reversible Logic,''
PhilArchive, 2025.

\bibitem{kohashiguchi2024axioms}
H. Kohashiguchi,
``Minimal Axioms for Quantum Structure,''
PhilArchive, 2025.

\bibitem{kohashiguchi2024naturalness}
H. Kohashiguchi,
``Algorithmic Naturalness on a Quantum Substrate,''
PhilArchive, 2025.

\bibitem{chaitin1975}
G. J. Chaitin,
``A theory of program size formally identical to information theory,''
\emph{J. ACM}, vol. 22, no. 3, pp. 329--340, 1975.

\bibitem{wolfram2002}
S. Wolfram,
\emph{A New Kind of Science},
Wolfram Media, 2002.

\bibitem{wolfram2020}
S. Wolfram,
``A Class of Models with the Potential to Represent Fundamental Physics,''
\emph{Complex Systems}, vol. 29, pp. 107--536, 2020.

\bibitem{lloyd2006}
S. Lloyd,
\emph{Programming the Universe: A Quantum Computer Scientist Takes on the Cosmos},
Knopf, 2006.

\bibitem{lenski2003}
R. E. Lenski, C. Ofria, R. T. Pennock, and C. Adami,
``The evolutionary origin of complex features,''
\emph{Nature}, vol. 423, pp. 139--144, 2003.

\bibitem{koza1992}
J. R. Koza,
\emph{Genetic Programming: On the Programming of Computers by Means of Natural Selection},
MIT Press, 1992.

\bibitem{smolin1992}
L. Smolin,
``Did the Universe Evolve?,''
\emph{Classical and Quantum Gravity}, vol. 9, no. 1, pp. 173--191, 1992.

\bibitem{smolin1997}
L. Smolin,
\emph{The Life of the Cosmos},
Oxford University Press, 1997.

\bibitem{bekenstein1981}
J. D. Bekenstein,
``Universal upper bound on the entropy-to-energy ratio for bounded systems,''
\emph{Physical Review D}, vol. 23, no. 2, pp. 287--298, 1981.

\bibitem{linde1986}
A. Linde,
``Eternally existing self-reproducing chaotic inflationary universe,''
\emph{Physics Letters B}, vol. 175, no. 4, pp. 395--400, 1986.

\bibitem{guth1981}
A. H. Guth,
``Inflationary universe: A possible solution to the horizon and flatness problems,''
\emph{Physical Review D}, vol. 23, no. 2, pp. 347--356, 1981.

\bibitem{guth1997}
A. H. Guth,
\emph{The Inflationary Universe: The Quest for a New Theory of Cosmic Origins},
Perseus Books, 1997.

\end{thebibliography}

\end{document}
